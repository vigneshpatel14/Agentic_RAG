{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8faf100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, Annotated\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import PersistentClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "014c031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_PROJECT\"] = \"HR_RAG_PROJECT\"\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "636c6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_pdf(directory_path):\n",
    "    all_docs = []\n",
    "    pdf_files = list(Path(directory_path).rglob(\"*.pdf\"))\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"No PDF files found in:\", directory_path)\n",
    "        return []\n",
    "\n",
    "    for file_path in pdf_files:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            doc.metadata.update({\"source_file\": file_path.name, \"file_type\": \"pdf\"})\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    print(f\"Loaded {len(all_docs)} documents from {directory_path}\")\n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "904db069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27383030",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "    def generate_embeddings(self, texts):\n",
    "        return self.embedding_model.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4cd476b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, persistent_directory=\"./chroma_db\"):\n",
    "        self.client = PersistentClient(path=persistent_directory)\n",
    "        self.collection = self.client.get_or_create_collection(name=\"PDF_DOCUMENTS\")\n",
    "\n",
    "    def add_documents(self, documents, embeddings):\n",
    "        ids = [str(uuid.uuid4()) for _ in documents]\n",
    "        metadatas = [doc.metadata for doc in documents]\n",
    "        documents_text = [doc.page_content for doc in documents]\n",
    "        embeddings_list = embeddings.tolist()\n",
    "\n",
    "        self.collection.add(\n",
    "            ids=ids,\n",
    "            documents=documents_text,\n",
    "            embeddings=embeddings_list,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "\n",
    "    def search(self, query_embedding, top_k=5):\n",
    "        return self.collection.query(query_embeddings=[query_embedding], n_results=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0be543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    def __init__(self, vectorstore, embedding_manager):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query, top_k=5, min_score=0.0):\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        results = self.vectorstore.search(query_embedding, top_k=top_k)\n",
    "\n",
    "        docs, metas, dists = results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]\n",
    "        retrieved = []\n",
    "\n",
    "        for i, (doc, meta, dist) in enumerate(zip(docs, metas, dists)):\n",
    "            similarity_score = 1 - dist\n",
    "            if similarity_score >= min_score:\n",
    "                retrieved.append({\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    \"content\": doc,\n",
    "                    \"metadata\": meta,\n",
    "                    \"similarity_score\": similarity_score,\n",
    "                    \"rank\": i + 1\n",
    "                })\n",
    "        return retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad175ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqLLM:\n",
    "    def __init__(self, api_key):\n",
    "        self.llm = ChatGroq(temperature=0, model=\"gemma2-9b-it\", api_key=api_key)\n",
    "\n",
    "    def generate_response(self, question, context):\n",
    "        system_prompt = f\"\"\"\n",
    "        You are a helpful AI. Use the provided context to answer.\n",
    "        If context is insufficient, say you don't know.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [SystemMessage(content=system_prompt), HumanMessage(content=question)]\n",
    "        return self.llm.invoke(messages).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d790f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_simple(query, retriever, llm, top_k=5):\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join(r['content'] for r in results)\n",
    "    return llm.generate_response(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2372109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 378 documents from ./data\n"
     ]
    }
   ],
   "source": [
    "docs = process_all_pdf(\"./data\")\n",
    "chunks = split_docs(docs)\n",
    "embedding_manager = EmbeddingManager()\n",
    "embeddings = embedding_manager.generate_embeddings([c.page_content for c in chunks])\n",
    "vectorstore = VectorStore()\n",
    "vectorstore.add_documents(chunks, embeddings)\n",
    "\n",
    "retriever = RAGRetriever(vectorstore, embedding_manager)\n",
    "llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a88e5b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text is just the beginning of O. Henry's short story \"The Gift of the Magi.\"  \n",
      "\n",
      "Here's a summary of the story:\n",
      "\n",
      "Della and Jim are a young, poor couple deeply in love. They each have something precious to them: Della has her beautiful, long hair, and Jim has a gold pocket watch, a family heirloom.  \n",
      "\n",
      "Christmas is approaching, and they both desperately want to buy each other the perfect gift. Della, with only $1.87 saved, decides to sell her hair to buy Jim a platinum chain for his watch.  \n",
      "\n",
      "Meanwhile, Jim, having saved up for months, sells his watch to buy Della a set of beautiful combs for her hair. \n",
      "\n",
      "When they exchange gifts on Christmas morning, they are both heartbroken to realize that their sacrifices have made the gifts useless. Della's hair is gone, and Jim's watch is sold. \n",
      "\n",
      "However, they understand each other's love and sacrifice, and the story ends with a poignant reflection on the true meaning of Christmas: the selfless love and generosity that transcends material possessions. \n",
      "\n",
      "\n",
      "Let me know if you'd like me to elaborate on any part of the story or if you have any other questions. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = rag_simple(\"Can you tell me The Gift of the Magi story\", retriever, llm)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64096ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c623d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "appshs=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "159b2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_graph = ChatGroq(model=\"gemma2-9b-it\", temperature=0.6)\n",
    "\n",
    "tool = TavilySearch(max_results=3)\n",
    "tools = [tool]\n",
    "\n",
    "llm_with_tools = llm_graph.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8584b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_calling_llm(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25d731bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\",\n",
    "    tools_condition,\n",
    "    {\"tools\": \"tools\", END: END}\n",
    ")\n",
    "\n",
    "\n",
    "builder.add_edge(\"tools\", \"tool_calling_llm\")\n",
    "\n",
    "\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24ca4e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sundar Pichai\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "\n",
    "state = {\"messages\": [\"Who is the CEO of Google?\"]}\n",
    "result = graph.invoke(state, config)\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ebb802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_hr_assistant(user_query: str):\n",
    "    \n",
    "    retrieved_docs = retriever.retrieve(user_query, top_k=3)\n",
    "    context = \"\\n\\n\".join([doc[\"content\"] for doc in retrieved_docs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an HR assistant. Use the following HR policy context to answer the question.\n",
    "    If unsure, say \"I don't know\" rather than making up answers.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {user_query}\n",
    "    \"\"\"\n",
    "    return graph.invoke({\"messages\": [(\"user\", prompt)]}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5e31435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What are the vacation policies?\"\n",
    "result = query_hr_assistant(test_query)\n",
    "print(result[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HR_RAG_PROJECT (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
